train_micro_batch_size_per_gpu: 4
gradient_accumulation_steps: 1
optimizer:
  type: AdamW
  params:
    lr: 5.0e-05
    betas:
      - 0.9
      - 0.95
    eps: 1.0e-08
    weight_decay: 0.1
fp16:
  enabled: false
  autocast: true
gradient_clipping: 1.0
zero_optimization:
  stage: 2
  contiguous_gradients: true
  overlap_comm: true
steps_per_print: 1.0e+10
mcpt:
  warmup_tokens: 0
  decay_tokens: 8.0e+5