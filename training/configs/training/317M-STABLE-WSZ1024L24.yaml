train_micro_batch_size_per_gpu: 4
gradient_accumulation_steps: 1
optimizer:
  type: AdamW
  params:
    lr: 8.0e-05
    betas:
      - 0.9
      - 0.95
    eps: 1.0e-08
    weight_decay: 0.1
fp16:
  enabled: true
  autocast: true
gradient_clipping: 1.0
zero_optimization:
  stage: 0
steps_per_print: 1.0e+10
warmup_tokens: 2.0e+5
decay_tokens: 4.0e+5