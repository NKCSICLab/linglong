train_micro_batch_size_per_gpu: 2
gradient_accumulation_steps: 1
optimizer:
  type: AdamW
  params:
    lr: 1.5e-04
    betas:
      - 0.9
      - 0.95
    eps: 1.0e-08
    weight_decay: 0.1
fp16:
  enabled: true
  autocast: true
gradient_clipping: 1.0
zero_optimization:
  stage: 3
  offload_optimizer:
    device: cpu
  offload_param:
    device: cpu
  contiguous_gradients: true
  overlap_comm: true
steps_per_print: 1.0e+10
mcpt:
  warmup_tokens: 6.0e+8
  decay_tokens: 1.06e+10